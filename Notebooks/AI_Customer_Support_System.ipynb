{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/939RMQUBtlbBLUuQFy8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakshithasr987/LABORATORY1/blob/main/Notebooks/AI_Customer_Support_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End AI Solution: Research on Improving the Quality of Online Customer Support Using Artificial Intelligence\n",
        "\n",
        "**Author:** Rakshitha Shyamanur Revanaradhya\n",
        "**Date:** November 2025  \n",
        "**Lab:** 1.5 - Demonstration of an End-to-End AI Solution using Gemini API\n",
        "\n",
        "## System Description\n",
        "\n",
        "This notebook demonstrates a complete AI agent system for [brief description of your thesis project, e.g., \"automated customer support analysis\", \"research paper summarization\", \"medical diagnosis assistance\", etc.].\n",
        "\n",
        "**Key Features:**\n",
        "- Uses Google's Gemini API for natural language processing\n",
        "- Implements prompt engineering techniques from Lab 1.4\n",
        "- Demonstrates end-to-end pipeline: Input ‚Üí Processing ‚Üí Reasoning ‚Üí Output\n",
        "\n",
        "**Problem Statement:**\n",
        "[Describe what problem your AI system solves - 2-3 sentences]"
      ],
      "metadata": {
        "id": "BPurSW_8yIrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nKY9Cl87yLXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Environment Setup\n",
        "\n",
        "Installing required libraries and setting up the Gemini API client."
      ],
      "metadata": {
        "id": "FVw_IKTUyNCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Google Generative AI library\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "# Import required libraries\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úÖ Libraries installed and imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uTLX4SQyOS_",
        "outputId": "938fcdc9-f817-4b75-e9c7-963659ab00a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries installed and imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Configure Gemini API\n",
        "\n",
        "Loading the API key securely from Colab Secrets and initializing the Gemini model."
      ],
      "metadata": {
        "id": "5-gytxqVyUIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API key from Colab Secrets (secure method)\n",
        "try:\n",
        "    GEMINI_KEY = userdata.get('GEMINI_KEY')\n",
        "    print(\"‚úÖ API key loaded successfully from secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading API key: {e}\")\n",
        "    print(\"Please add GEMINI_KEY to Colab Secrets (Tools ‚Üí Secrets)\")\n",
        "\n",
        "# Configure the Gemini API\n",
        "genai.configure(api_key=GEMINI_KEY)\n",
        "\n",
        "# Initialize the model (using Gemini 1.5 Flash for faster responses)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "print(\"‚úÖ Gemini model initialized successfully!\")\n",
        "print(f\"Model name: {model.model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIH8QT-DyWfj",
        "outputId": "bb4a9ccd-aa7f-4390-910d-76befa3b3a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API key loaded successfully from secrets\n",
            "‚úÖ Gemini model initialized successfully!\n",
            "Model name: models/gemini-1.5-flash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load Prompt Engineering Examples from Lab 1.4\n",
        "\n",
        "These are the two prompt examples developed in the previous lab, demonstrating different aspects of the AI system."
      ],
      "metadata": {
        "id": "eefEj2w-yaC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT 1: [Describe what this prompt does]\n",
        "# Example: \"Initial data analysis and classification\"\n",
        "\n",
        "prompt_1 = \"\"\"\n",
        "[PASTE YOUR FIRST PROMPT FROM LAB 1.4 HERE]\n",
        "\n",
        "Example structure:\n",
        "You are an AI assistant specialized in [your domain].\n",
        "\n",
        "Task: [What you want the AI to do]\n",
        "\n",
        "Input: {input_text}\n",
        "\n",
        "Please provide:\n",
        "1. Analysis of the input\n",
        "2. Key findings\n",
        "3. Recommended action\n",
        "\n",
        "Output format: [Specify JSON, text, etc.]\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù Prompt 1 loaded successfully\")\n",
        "print(f\"Prompt length: {len(prompt_1)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV1H5JWiycXY",
        "outputId": "cf72d13e-f573-496b-a1ec-f6ad1da85601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Prompt 1 loaded successfully\n",
            "Prompt length: 299 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT 2: [Describe what this prompt does]\n",
        "# Example: \"Detailed reasoning and recommendation generation\"\n",
        "\n",
        "prompt_2 = \"\"\"\n",
        "[PASTE YOUR SECOND PROMPT FROM LAB 1.4 HERE]\n",
        "\n",
        "Example structure:\n",
        "Based on the following analysis, provide detailed recommendations.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Requirements:\n",
        "- Explain your reasoning step-by-step\n",
        "- Provide confidence levels\n",
        "- Suggest alternative approaches\n",
        "\n",
        "Format your response with clear sections.\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù Prompt 2 loaded successfully\")\n",
        "print(f\"Prompt length: {len(prompt_2)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hATtzbGtygq4",
        "outputId": "52ad46fb-7942-4a30-bc8a-702f07f5b67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Prompt 2 loaded successfully\n",
            "Prompt length: 310 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: End-to-End Pipeline Demonstration\n",
        "\n",
        "This section demonstrates the complete AI system workflow:\n",
        "1. **Input Stage**: Accept and validate input data\n",
        "2. **Data Understanding**: Analyze and preprocess the input\n",
        "3. **Reasoning**: Apply AI model with engineered prompts\n",
        "4. **Inference**: Generate intermediate results\n",
        "5. **Output Generation**: Produce final formatted output"
      ],
      "metadata": {
        "id": "VKF5w0FWyjtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the End-to-End Pipeline Class\n",
        "\n",
        "class AISystemPipeline:\n",
        "    \"\"\"\n",
        "    Complete AI system pipeline integrating Gemini API and prompt engineering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.history = []\n",
        "\n",
        "    def stage_1_input(self, user_input):\n",
        "        \"\"\"Stage 1: Accept and validate input\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"STAGE 1: INPUT PROCESSING\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"üì• Raw Input: {user_input[:100]}...\")\n",
        "\n",
        "        # Basic validation\n",
        "        if not user_input or len(user_input.strip()) == 0:\n",
        "            raise ValueError(\"Empty input provided\")\n",
        "\n",
        "        self.history.append({\n",
        "            'stage': 'input',\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'data': user_input\n",
        "        })\n",
        "\n",
        "        print(\"‚úÖ Input validated\\n\")\n",
        "        return user_input\n",
        "\n",
        "    def stage_2_understanding(self, input_data):\n",
        "        \"\"\"Stage 2: Data understanding and preprocessing\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"STAGE 2: DATA UNDERSTANDING\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Use Prompt 1 for initial analysis\n",
        "        formatted_prompt = prompt_1.replace(\"{input_text}\", input_data)\n",
        "\n",
        "        print(\"ü§î Analyzing input with Gemini (Prompt 1)...\")\n",
        "        response = self.model.generate_content(formatted_prompt)\n",
        "        understanding = response.text\n",
        "\n",
        "        print(f\"üìä Analysis Result:\\n{understanding[:300]}...\\n\")\n",
        "\n",
        "        self.history.append({\n",
        "            'stage': 'understanding',\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'prompt_used': 'prompt_1',\n",
        "            'output': understanding\n",
        "        })\n",
        "\n",
        "        return understanding\n",
        "\n",
        "    def stage_3_reasoning(self, understanding_output):\n",
        "        \"\"\"Stage 3: Deep reasoning and inference\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"STAGE 3: REASONING & INFERENCE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Use Prompt 2 for detailed reasoning\n",
        "        formatted_prompt = prompt_2.replace(\"{context}\", understanding_output)\n",
        "\n",
        "        print(\"üß† Applying reasoning with Gemini (Prompt 2)...\")\n",
        "        response = self.model.generate_content(formatted_prompt)\n",
        "        reasoning = response.text\n",
        "\n",
        "        print(f\"üí° Reasoning Result:\\n{reasoning[:300]}...\\n\")\n",
        "\n",
        "        self.history.append({\n",
        "            'stage': 'reasoning',\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'prompt_used': 'prompt_2',\n",
        "            'output': reasoning\n",
        "        })\n",
        "\n",
        "        return reasoning\n",
        "\n",
        "    def stage_4_output(self, reasoning_result):\n",
        "        \"\"\"Stage 4: Generate final formatted output\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"STAGE 4: OUTPUT GENERATION\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Format the final output\n",
        "        final_output = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'status': 'completed',\n",
        "            'result': reasoning_result,\n",
        "            'pipeline_stages': len(self.history)\n",
        "        }\n",
        "\n",
        "        print(\"üì§ Final Output Generated Successfully!\")\n",
        "        print(f\"‚úÖ Pipeline completed with {len(self.history)} stages\\n\")\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    def run_pipeline(self, user_input):\n",
        "        \"\"\"Execute the complete end-to-end pipeline\"\"\"\n",
        "        print(\"\\n\" + \"üöÄ \" * 30)\n",
        "        print(\"STARTING END-TO-END AI PIPELINE\")\n",
        "        print(\"üöÄ \" * 30 + \"\\n\")\n",
        "\n",
        "        try:\n",
        "            # Execute all stages\n",
        "            validated_input = self.stage_1_input(user_input)\n",
        "            understanding = self.stage_2_understanding(validated_input)\n",
        "            reasoning = self.stage_3_reasoning(understanding)\n",
        "            final_output = self.stage_4_output(reasoning)\n",
        "\n",
        "            print(\"\\n\" + \"‚úÖ \" * 30)\n",
        "            print(\"PIPELINE EXECUTION COMPLETED SUCCESSFULLY\")\n",
        "            print(\"‚úÖ \" * 30 + \"\\n\")\n",
        "\n",
        "            return final_output\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Pipeline Error: {e}\")\n",
        "            return None\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = AISystemPipeline(model)\n",
        "print(\"‚úÖ AI System Pipeline initialized and ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHRj6-Dcylxw",
        "outputId": "841c1f64-4478-4ecc-ebb8-c3699a84c963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AI System Pipeline initialized and ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Execute the Pipeline with Example Input\n",
        "\n",
        "Now we'll run the complete system with a test input to demonstrate all stages."
      ],
      "metadata": {
        "id": "vCAyZZAxyu66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API key from Colab Secrets (secure method)\n",
        "try:\n",
        "    GEMINI_KEY = userdata.get('GEMINI_KEY')\n",
        "    print(\"‚úÖ API key loaded successfully from secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading API key: {e}\")\n",
        "    print(\"Please add GEMINI_KEY to Colab Secrets (Tools ‚Üí Secrets)\")\n",
        "\n",
        "# Configure the Gemini API\n",
        "genai.configure(api_key=GEMINI_KEY)\n",
        "\n",
        "# List available models to verify\n",
        "print(\"\\nüìã Checking available Gemini models...\\n\")\n",
        "try:\n",
        "    available_models = []\n",
        "    for m in genai.list_models():\n",
        "        if 'generateContent' in m.supported_generation_methods:\n",
        "            available_models.append(m.name)\n",
        "            print(f\"‚úÖ {m.name}\")\n",
        "    print(f\"\\n‚úÖ Found {len(available_models)} models that support generateContent\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not list models: {e}\\n\")\n",
        "\n",
        "# Initialize the model with correct name format\n",
        "# Use one of these options (try them in order):\n",
        "\n",
        "# OPTION 1: Try Gemini 1.5 Flash (recommended - fast and efficient)\n",
        "try:\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "    print(\"‚úÖ Gemini 1.5 Flash model initialized successfully!\")\n",
        "    print(f\"Model name: {model.model_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Option 1 failed: {e}\")\n",
        "\n",
        "    # OPTION 2: Try Gemini 1.5 Pro\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
        "        print(\"‚úÖ Gemini 1.5 Pro model initialized successfully!\")\n",
        "        print(f\"Model name: {model.model_name}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Option 2 failed: {e2}\")\n",
        "\n",
        "        # OPTION 3: Try Gemini Pro (older version)\n",
        "        try:\n",
        "            model = genai.GenerativeModel('gemini-pro')\n",
        "            print(\"‚úÖ Gemini Pro model initialized successfully!\")\n",
        "            print(f\"Model name: {model.model_name}\")\n",
        "        except Exception as e3:\n",
        "            print(f\"‚ùå All options failed. Please check your API key.\")\n",
        "            raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "S2et3ZtP7Ktu",
        "outputId": "7f4b1e9c-fdb7-4722-a09d-a9478856534d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API key loaded successfully from secrets\n",
            "\n",
            "üìã Checking available Gemini models...\n",
            "\n",
            "‚úÖ models/gemini-2.5-pro-preview-03-25\n",
            "‚úÖ models/gemini-2.5-flash-preview-05-20\n",
            "‚úÖ models/gemini-2.5-flash\n",
            "‚úÖ models/gemini-2.5-flash-lite-preview-06-17\n",
            "‚úÖ models/gemini-2.5-pro-preview-05-06\n",
            "‚úÖ models/gemini-2.5-pro-preview-06-05\n",
            "‚úÖ models/gemini-2.5-pro\n",
            "‚úÖ models/gemini-2.0-flash-exp\n",
            "‚úÖ models/gemini-2.0-flash\n",
            "‚úÖ models/gemini-2.0-flash-001\n",
            "‚úÖ models/gemini-2.0-flash-lite-001\n",
            "‚úÖ models/gemini-2.0-flash-lite\n",
            "‚úÖ models/gemini-2.0-flash-lite-preview-02-05\n",
            "‚úÖ models/gemini-2.0-flash-lite-preview\n",
            "‚úÖ models/gemini-2.0-pro-exp\n",
            "‚úÖ models/gemini-2.0-pro-exp-02-05\n",
            "‚úÖ models/gemini-exp-1206\n",
            "‚úÖ models/gemini-2.0-flash-thinking-exp-01-21\n",
            "‚úÖ models/gemini-2.0-flash-thinking-exp\n",
            "‚úÖ models/gemini-2.0-flash-thinking-exp-1219\n",
            "‚úÖ models/gemini-2.5-flash-preview-tts\n",
            "‚úÖ models/gemini-2.5-pro-preview-tts\n",
            "‚úÖ models/learnlm-2.0-flash-experimental\n",
            "‚úÖ models/gemma-3-1b-it\n",
            "‚úÖ models/gemma-3-4b-it\n",
            "‚úÖ models/gemma-3-12b-it\n",
            "‚úÖ models/gemma-3-27b-it\n",
            "‚úÖ models/gemma-3n-e4b-it\n",
            "‚úÖ models/gemma-3n-e2b-it\n",
            "‚úÖ models/gemini-flash-latest\n",
            "‚úÖ models/gemini-flash-lite-latest\n",
            "‚úÖ models/gemini-pro-latest\n",
            "‚úÖ models/gemini-2.5-flash-lite\n",
            "‚úÖ models/gemini-2.5-flash-image-preview\n",
            "‚úÖ models/gemini-2.5-flash-image\n",
            "‚úÖ models/gemini-2.5-flash-preview-09-2025\n",
            "‚úÖ models/gemini-2.5-flash-lite-preview-09-2025\n",
            "‚úÖ models/gemini-3-pro-preview\n",
            "‚úÖ models/gemini-robotics-er-1.5-preview\n",
            "‚úÖ models/gemini-2.5-computer-use-preview-10-2025\n",
            "\n",
            "‚úÖ Found 40 models that support generateContent\n",
            "\n",
            "‚úÖ Gemini 1.5 Flash model initialized successfully!\n",
            "Model name: models/gemini-1.5-flash-latest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Demonstrate Individual Prompts\n",
        "\n",
        "Testing each prompt separately to show their specific capabilities."
      ],
      "metadata": {
        "id": "CtFPipFw8Crk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API key from Colab Secrets\n",
        "try:\n",
        "    GEMINI_KEY = userdata.get('GEMINI_KEY')\n",
        "    print(\"‚úÖ API key loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading API key: {e}\")\n",
        "    raise\n",
        "\n",
        "# Configure the Gemini API\n",
        "genai.configure(api_key=GEMINI_KEY)\n",
        "\n",
        "# Get the first available model dynamically\n",
        "print(\"üîç Finding available model...\")\n",
        "available_model = None\n",
        "\n",
        "for m in genai.list_models():\n",
        "    if 'generateContent' in m.supported_generation_methods:\n",
        "        available_model = m.name\n",
        "        print(f\"‚úÖ Found model: {available_model}\")\n",
        "        break\n",
        "\n",
        "if not available_model:\n",
        "    raise Exception(\"No models available for generateContent!\")\n",
        "\n",
        "# Initialize with the found model\n",
        "model = genai.GenerativeModel(model_name=available_model)\n",
        "print(f\"‚úÖ Model initialized: {model.model_name}\")\n",
        "\n",
        "# Test it\n",
        "try:\n",
        "    test = model.generate_content(\"Say 'Hello!'\")\n",
        "    print(f\"‚úÖ Test successful: {test.text}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "xAPKhxp49Y7A",
        "outputId": "14477248-e5e7-4b14-c8ba-88f2f23eeb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API key loaded successfully\n",
            "üîç Finding available model...\n",
            "‚úÖ Found model: models/gemini-2.5-pro-preview-03-25\n",
            "‚úÖ Model initialized: models/gemini-2.5-pro-preview-03-25\n",
            "‚ùå Test failed: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro-exp\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro-exp\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro-exp\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro-exp\n",
            "Please retry in 12.674018681s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 306.72ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.api_core import retry\n",
        "from google.api_core import exceptions\n",
        "\n",
        "def safe_generate_content(model, prompt, max_retries=3):\n",
        "    \"\"\"\n",
        "    Safely generate content with automatic retry on rate limits\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            return response\n",
        "\n",
        "        except exceptions.TooManyRequests as e:\n",
        "            # Rate limit hit\n",
        "            if attempt < max_retries - 1:\n",
        "                # Extract wait time from error message\n",
        "                wait_time = 10 * (attempt + 1)  # 10, 20, 30 seconds\n",
        "                print(f\"‚ö†Ô∏è  Rate limit exceeded. Waiting {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(\"‚ùå Max retries reached. Please wait a minute and try again.\")\n",
        "                raise\n",
        "\n",
        "        except exceptions.ResourceExhausted as e:\n",
        "            # Quota exceeded\n",
        "            print(f\"‚ùå Quota exceeded: {e}\")\n",
        "            print(\"üí° Switch to a free tier model like 'gemini-1.5-flash'\")\n",
        "            raise\n",
        "\n",
        "        except Exception as e:\n",
        "            # Other errors\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            raise\n",
        "\n",
        "    return None\n",
        "\n",
        "# Usage example:\n",
        "# response = safe_generate_content(model, \"Your prompt here\")"
      ],
      "metadata": {
        "id": "nMcJtiiz_aGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Pipeline Visualization\n",
        "\n",
        "Visual representation of the system flow and execution history."
      ],
      "metadata": {
        "id": "vq3yC9bk_eO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the pipeline execution history\n",
        "import pandas as pd\n",
        "\n",
        "# Create a summary DataFrame\n",
        "if pipeline.history:\n",
        "    history_df = pd.DataFrame([\n",
        "        {\n",
        "            'Stage Number': i + 1,\n",
        "            'Stage Name': entry['stage'].upper(),\n",
        "            'Timestamp': entry['timestamp'],\n",
        "            'Prompt Used': entry.get('prompt_used', 'N/A'),\n",
        "            'Output Length': len(str(entry.get('output', entry.get('data', ''))))\n",
        "        }\n",
        "        for i, entry in enumerate(pipeline.history)\n",
        "    ])\n",
        "\n",
        "    print(\"üìä PIPELINE EXECUTION SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(history_df.to_string(index=False))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Show stage flow\n",
        "    print(\"üîÑ STAGE FLOW:\")\n",
        "    for i, entry in enumerate(pipeline.history):\n",
        "        arrow = \"‚Üí\" if i < len(pipeline.history) - 1 else \"\"\n",
        "        print(f\"  [{entry['stage'].upper()}] {arrow}\", end=\" \")\n",
        "    print(\"\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No pipeline history available. Please run the pipeline first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iuPtr65_e_R",
        "outputId": "d7f5905e-db51-4eae-c916-480107d33cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä PIPELINE EXECUTION SUMMARY\n",
            "============================================================\n",
            " Stage Number Stage Name                  Timestamp Prompt Used  Output Length\n",
            "            1      INPUT 2025-11-18T16:27:41.551907         N/A            448\n",
            "\n",
            "\n",
            "üîÑ STAGE FLOW:\n",
            "  [INPUT]  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection and Analysis\n",
        "\n",
        "### What This System Does\n",
        "\n",
        "[Write 5-10 sentences explaining your complete AI agent - what it does, how it works, what problem it solves]\n",
        "\n",
        "### Use of Gemini and Prompt Engineering\n",
        "\n",
        "[Explain how you used Gemini API and what prompt engineering techniques you applied from Lab 1.4]\n",
        "\n",
        "### What Worked Well\n",
        "\n",
        "1. [Point 1]\n",
        "2. [Point 2]\n",
        "3. [Point 3]\n",
        "\n",
        "### Areas for Improvement\n",
        "\n",
        "1. [Improvement 1]\n",
        "2. [Improvement 2]\n",
        "3. [Improvement 3]\n",
        "\n",
        "### Future Enhancements\n",
        "\n",
        "[Describe potential future improvements to your system]"
      ],
      "metadata": {
        "id": "QbRLRYJF_nSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final system statistics and information\n",
        "print(\"=\" * 60)\n",
        "print(\"SYSTEM INFORMATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Gemini Model: {model.model_name}\")\n",
        "print(f\"‚úÖ Total Pipeline Stages: 4\")\n",
        "print(f\"‚úÖ Prompts Implemented: 2\")\n",
        "print(f\"‚úÖ Execution History Entries: {len(pipeline.history)}\")\n",
        "print(f\"‚úÖ Notebook Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "xTBW8QrJ_oaW",
        "outputId": "cd0ebc93-57e2-4f3b-f0cf-8e59f9de0fce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SYSTEM INFORMATION\n",
            "============================================================\n",
            "‚úÖ Gemini Model: models/gemini-2.5-pro-preview-03-25\n",
            "‚úÖ Total Pipeline Stages: 4\n",
            "‚úÖ Prompts Implemented: 2\n",
            "‚úÖ Execution History Entries: 1\n",
            "‚úÖ Notebook Completed: 2025-11-18 17:24:00\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}